{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokenization Playground\n",
        "\n",
        "Use this notebook to explore how different tokenizers split your documents.\n",
        "\n",
        "- Uses your `ingestion.py` (via `MinimalProcessor`) to parse the MSMARCO TSV.\n",
        "- Compares `SimpleTokenizer`, our `SpacyTokenizer`, and a customizable spaCy tokenizer.\n",
        "- Adjustable spaCy options: model, disabled pipeline components, lowercasing/ASCII folding, infix rules, URL/email handling.\n",
        "\n",
        "Note: If `ipywidgets` isn't installed, the notebook falls back to non-interactive cells with defaults.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display settings to avoid truncation\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.width', None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and setup\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import json\n",
        "from typing import Any, Optional\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from ingestion import MinimalProcessor, Columns\n",
        "from tokenization import SimpleTokenizer, SpacyTokenizer\n",
        "\n",
        "# Optional: widgets\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display\n",
        "    HAS_WIDGETS = True\n",
        "except Exception:\n",
        "    HAS_WIDGETS = False\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "DOCUMENTS_PATH = os.getenv(\"DOCUMENTS\", \"msmarco-docs.tsv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 100 docs from msmarco-docs.tsv\n"
          ]
        }
      ],
      "source": [
        "# Load a sample of documents using MinimalProcessor\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def parse_tsv_line(line: str) -> Optional[list[str]]:\n",
        "    parts = line.rstrip(\"\\n\").split(\"\\t\")\n",
        "    if len(parts) != 4:\n",
        "        return None\n",
        "    return parts\n",
        "\n",
        "\n",
        "def load_documents(path: str | Path, limit: int = 50, skip: int = 0) -> list[dict[str, Any]]:\n",
        "    docs: list[dict[str, Any]] = []\n",
        "    proc = MinimalProcessor()\n",
        "    added = 0\n",
        "    path = Path(path)\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"Documents file not found: {path}\")\n",
        "\n",
        "    with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        # skip\n",
        "        for _ in range(skip):\n",
        "            if not f.readline():\n",
        "                break\n",
        "        for line in f:\n",
        "            parsed = parse_tsv_line(line)\n",
        "            if not parsed:\n",
        "                continue\n",
        "            doc_id = parsed[0]\n",
        "            _, payload = proc.process(doc_id, parsed)\n",
        "            try:\n",
        "                obj = json.loads(payload)\n",
        "                docs.append(obj)\n",
        "            except json.JSONDecodeError:\n",
        "                # Should not happen, but skip invalid\n",
        "                continue\n",
        "            added += 1\n",
        "            if added >= limit:\n",
        "                break\n",
        "    return docs\n",
        "\n",
        "\n",
        "try:\n",
        "    documents: list[dict[str, Any]] = load_documents(DOCUMENTS_PATH, limit=100)\n",
        "    print(f\"Loaded {len(documents)} docs from {DOCUMENTS_PATH}\")\n",
        "except Exception as e:\n",
        "    documents = []\n",
        "    print(f\"Failed to load documents: {e}\")\n",
        "\n",
        "# Helper to get combined text\n",
        "\n",
        "def get_document_text(doc: dict[str, Any], include_title: bool = True, include_body: bool = True) -> str:\n",
        "    title = str(doc.get(Columns.title.value, \"\")) if include_title else \"\"\n",
        "    body = str(doc.get(Columns.body.value, \"\")) if include_body else \"\"\n",
        "    sep = \"\\n\\n\" if title and body else \"\"\n",
        "    return f\"{title}{sep}{body}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenizer builders and comparison utilities\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class SpacyOptions:\n",
        "    model: str = \"blank\"            # \"blank\" for spacy.blank(\"en\"), or a model name like \"en_core_web_sm\"\n",
        "    disable: list[str] | None = None # pipeline components to disable when loading a model\n",
        "    lowercase: bool = True\n",
        "    ascii_fold: bool = True\n",
        "    add_url_rule: bool = True\n",
        "    add_email_rule: bool = True\n",
        "    custom_infixes: list[str] | None = None  # e.g. [r\"\\.\", r\"-\", r\"_\", r\"\\/\", r\"\\:\"]\n",
        "\n",
        "\n",
        "def build_spacy_tokenizer(opts: SpacyOptions) -> SpacyTokenizer:\n",
        "    \"\"\"Build our SpacyTokenizer honoring basic normalization and tokenizer rules.\n",
        "    If opts.model == \"blank\", we create spacy.blank(\"en\"); otherwise spacy.load.\n",
        "    We optionally alter tokenizer URL/email matching and infix rules.\n",
        "    \"\"\"\n",
        "    import spacy\n",
        "    if opts.model in (None, \"\", \"blank\"):\n",
        "        nlp = spacy.blank(\"en\")\n",
        "    else:\n",
        "        try:\n",
        "            nlp = spacy.load(opts.model, disable=opts.disable or [\n",
        "                \"tagger\", \"parser\", \"ner\", \"lemmatizer\", \"attribute_ruler\", \"textcat\"\n",
        "            ])\n",
        "        except Exception:\n",
        "            nlp = spacy.blank(\"en\")\n",
        "\n",
        "    # Customize tokenizer rules if requested\n",
        "    from spacy.lang.char_classes import LIST_ELLIPSES, LIST_ICONS\n",
        "    from spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER\n",
        "    from spacy.tokenizer import Tokenizer\n",
        "    from spacy.util import compile_infix_regex\n",
        "\n",
        "    # Start with default tokenizer\n",
        "    tokenizer = nlp.tokenizer\n",
        "\n",
        "    # Adjust URL/email matching via token_match\n",
        "    token_match = tokenizer.token_match\n",
        "    if opts.add_url_rule or opts.add_email_rule:\n",
        "        import re\n",
        "        url_pattern = r\"https?://\\S+\" if opts.add_url_rule else None\n",
        "        email_pattern = r\"[\\w.+-]+@[\\w-]+\\.[\\w.-]+\" if opts.add_email_rule else None\n",
        "        patterns = [p for p in [url_pattern, email_pattern] if p]\n",
        "        if patterns:\n",
        "            combined = re.compile(\"|\".join(patterns))\n",
        "            def _token_match(text: str) -> Optional[re.Match[str]]:  # type: ignore\n",
        "                m = combined.match(text)\n",
        "                return m if m else (token_match(text) if token_match else None)\n",
        "            tokenizer.token_match = _token_match  # type: ignore\n",
        "\n",
        "    # Adjust infixes if provided\n",
        "    if opts.custom_infixes is not None:\n",
        "        infixes = tuple(opts.custom_infixes)\n",
        "        infix_re = compile_infix_regex(infixes)\n",
        "        tokenizer.infix_finditer = infix_re.finditer  # type: ignore\n",
        "\n",
        "    # Wrap into our SpacyTokenizer to leverage normalization and re-fallback\n",
        "    st = SpacyTokenizer(model=\"blank\", disable=None, lowercase=opts.lowercase, ascii_fold=opts.ascii_fold)\n",
        "    st.nlp = nlp\n",
        "    st.nlp.tokenizer = tokenizer\n",
        "    return st\n",
        "\n",
        "\n",
        "def compare_tokenizers(text: str, simple_tok: SimpleTokenizer, spacy_tok: SpacyTokenizer, custom_spacy_tok: SpacyTokenizer) -> dict[str, list[str]]:\n",
        "    return {\n",
        "        \"SimpleTokenizer\": simple_tok.tokenize(text),\n",
        "        \"SpacyTokenizer(default)\": spacy_tok.tokenize(text),\n",
        "        \"SpacyTokenizer(custom)\": custom_spacy_tok.tokenize(text),\n",
        "    }\n",
        "\n",
        "# Defaults\n",
        "default_simple = SimpleTokenizer(lowercase=True, ascii_fold=True)\n",
        "default_spacy = SpacyTokenizer(model=\"blank\", disable=None, lowercase=True, ascii_fold=True)\n",
        "\n",
        "# Example customizable options instance\n",
        "default_opts = SpacyOptions(\n",
        "    model=os.getenv(\"SPACY_MODEL\", \"blank\"),\n",
        "    disable=[c for c in (os.getenv(\"SPACY_DISABLE\", \"\").split(\",\") if os.getenv(\"SPACY_DISABLE\") else [])],\n",
        "    lowercase=True,\n",
        "    ascii_fold=True,\n",
        "    add_url_rule=True,\n",
        "    add_email_rule=True,\n",
        "    custom_infixes=[r\"\\.\", r\"-\", r\"_\", r\"\\/\", r\"\\:\"]\n",
        ")\n",
        "\n",
        "custom_spacy = build_spacy_tokenizer(default_opts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd8fdafc6b6046a193cfa36c96d73b00",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HBox(children=(IntSlider(value=0, description='doc idx', max=99), Checkbox(value=True, descript…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b94517be32214cca99e66d480724c923",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Interactive UI (if ipywidgets available)\n",
        "\n",
        "def render_results(text: str, simple_tok: SimpleTokenizer, spacy_tok: SpacyTokenizer, custom_spacy_tok: SpacyTokenizer):\n",
        "    # Show full document text\n",
        "    print(f\"Document text ({len(text)} chars):\\n{text}\\n\")\n",
        "    comp = compare_tokenizers(text, simple_tok, spacy_tok, custom_spacy_tok)\n",
        "    rows = []\n",
        "    for name, tokens in comp.items():\n",
        "        rows.append({\n",
        "            \"Tokenizer\": name,\n",
        "            \"Num tokens\": len(tokens),\n",
        "            \"Tokens\": \" \".join(tokens),\n",
        "        })\n",
        "    df = pd.DataFrame(rows)\n",
        "    display(df)\n",
        "\n",
        "if HAS_WIDGETS and documents:\n",
        "    # Controls\n",
        "    doc_index = widgets.IntSlider(value=0, min=0, max=max(0, len(documents)-1), step=1, description=\"doc idx\")\n",
        "    include_title = widgets.Checkbox(value=True, description=\"include title\")\n",
        "    include_body = widgets.Checkbox(value=True, description=\"include body\")\n",
        "\n",
        "    spacy_model = widgets.Text(value=os.getenv(\"SPACY_MODEL\", \"blank\"), description=\"spacy model\")\n",
        "    spacy_disable = widgets.Text(value=os.getenv(\"SPACY_DISABLE\", \"\"), description=\"disable\")\n",
        "    lowercase = widgets.Checkbox(value=True, description=\"lowercase\")\n",
        "    ascii_fold = widgets.Checkbox(value=True, description=\"ascii fold\")\n",
        "    add_url = widgets.Checkbox(value=True, description=\"url rule\")\n",
        "    add_email = widgets.Checkbox(value=True, description=\"email rule\")\n",
        "    infixes = widgets.Text(value=r\"\\.|-|_|\\/|\\:\", description=\"infixes regex |\")\n",
        "\n",
        "    out = widgets.Output()\n",
        "\n",
        "    def _update(_=None):\n",
        "        out.clear_output()\n",
        "        with out:\n",
        "            # Build current custom spacy tokenizer\n",
        "            disable_list = [c for c in spacy_disable.value.split(\",\") if c.strip()]\n",
        "            opts = SpacyOptions(\n",
        "                model=spacy_model.value.strip() or \"blank\",\n",
        "                disable=disable_list or None,\n",
        "                lowercase=lowercase.value,\n",
        "                ascii_fold=ascii_fold.value,\n",
        "                add_url_rule=add_url.value,\n",
        "                add_email_rule=add_email.value,\n",
        "                custom_infixes=[p for p in infixes.value.split(\"|\") if p]\n",
        "            )\n",
        "            custom_tok = build_spacy_tokenizer(opts)\n",
        "            text = get_document_text(documents[doc_index.value], include_title.value, include_body.value)\n",
        "            render_results(text, default_simple, default_spacy, custom_tok)\n",
        "\n",
        "    for w in [doc_index, include_title, include_body, spacy_model, spacy_disable, lowercase, ascii_fold, add_url, add_email, infixes]:\n",
        "        w.observe(_update, names=[\"value\"])  # type: ignore\n",
        "\n",
        "    control_box = widgets.VBox([\n",
        "        widgets.HBox([doc_index, include_title, include_body]),\n",
        "        widgets.HBox([spacy_model, spacy_disable]),\n",
        "        widgets.HBox([lowercase, ascii_fold, add_url, add_email]),\n",
        "        infixes,\n",
        "    ])\n",
        "    display(control_box, out)\n",
        "    _update()\n",
        "else:\n",
        "    print(\"Widgets are unavailable or no documents loaded. See the fallback cell below.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fallback demo (no widgets)\n",
        "if not HAS_WIDGETS:\n",
        "    examples: list[str] = []\n",
        "    if documents:\n",
        "        # Take first 3 documents' title+body\n",
        "        for i in range(min(3, len(documents))):\n",
        "            examples.append(get_document_text(documents[i], True, True))\n",
        "    else:\n",
        "        examples = [\n",
        "            \"Hello world! Email me at foo.bar+baz@example.com or visit https://spacy.io/usage.\",\n",
        "            \"SpaCy v3.8's tokenizer: test-case with hyphenated-words_and_tokens/segments: 12:30pm.\",\n",
        "        ]\n",
        "\n",
        "    # Build a couple of custom variants\n",
        "    opts_a = SpacyOptions(model=\"blank\", custom_infixes=[r\"\\.\", r\"-\", r\"_\"], add_url_rule=True, add_email_rule=True)\n",
        "    opts_b = SpacyOptions(model=\"blank\", custom_infixes=[r\"\\-\"], add_url_rule=False, add_email_rule=False, lowercase=False, ascii_fold=False)\n",
        "\n",
        "    custom_a = build_spacy_tokenizer(opts_a)\n",
        "    custom_b = build_spacy_tokenizer(opts_b)\n",
        "\n",
        "    for idx, text in enumerate(examples):\n",
        "        print(f\"\\n=== Example {idx} ===\\n{text}\\n\")\n",
        "        comp = {\n",
        "            \"SimpleTokenizer\": default_simple.tokenize(text),\n",
        "            \"SpacyTokenizer(default)\": default_spacy.tokenize(text),\n",
        "            \"SpacyTokenizer(custom A)\": custom_a.tokenize(text),\n",
        "            \"SpacyTokenizer(custom B)\": custom_b.tokenize(text),\n",
        "        }\n",
        "        df = pd.DataFrame({\n",
        "            \"Tokenizer\": comp.keys(),\n",
        "            \"Num tokens\": [len(v) for v in comp.values()],\n",
        "            \"Tokens\": [\" \".join(v) for v in comp.values()],\n",
        "        })\n",
        "        display(df)\n",
        "else:\n",
        "    print(\"Widgets available; use the interactive controls above.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
