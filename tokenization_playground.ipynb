{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokenization Playground\n",
        "\n",
        "Use this notebook to explore how different tokenizers split your documents.\n",
        "\n",
        "- Uses your `ingestion.py` (via `MinimalProcessor`) to parse the MSMARCO TSV.\n",
        "- Compares `SimpleTokenizer`, our `SpacyTokenizer`, and a customizable spaCy tokenizer.\n",
        "- Adjustable spaCy options: model, disabled pipeline components, lowercasing/ASCII folding, infix rules, URL/email handling.\n",
        "\n",
        "Note: If `ipywidgets` isn't installed, the notebook falls back to non-interactive cells with defaults.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display settings to avoid truncation\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.width', None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and setup\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import json\n",
        "from typing import Any, Optional\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from ingestion import MinimalProcessor, Columns\n",
        "from tokenization import SimpleTokenizer, SpacyTokenizer\n",
        "\n",
        "# Optional: widgets\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display\n",
        "    HAS_WIDGETS = True\n",
        "except Exception:\n",
        "    HAS_WIDGETS = False\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "DOCUMENTS_PATH = os.getenv(\"DOCUMENTS\", \"msmarco-docs.tsv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 100 docs from msmarco-docs.tsv\n"
          ]
        }
      ],
      "source": [
        "# Load a sample of documents using MinimalProcessor\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def parse_tsv_line(line: str) -> Optional[list[str]]:\n",
        "    parts = line.rstrip(\"\\n\").split(\"\\t\")\n",
        "    if len(parts) != 4:\n",
        "        return None\n",
        "    return parts\n",
        "\n",
        "\n",
        "def load_documents(path: str | Path, limit: int = 50, skip: int = 0) -> list[dict[str, Any]]:\n",
        "    docs: list[dict[str, Any]] = []\n",
        "    proc = MinimalProcessor()\n",
        "    added = 0\n",
        "    path = Path(path)\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"Documents file not found: {path}\")\n",
        "\n",
        "    with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        # skip\n",
        "        for _ in range(skip):\n",
        "            if not f.readline():\n",
        "                break\n",
        "        for line in f:\n",
        "            parsed = parse_tsv_line(line)\n",
        "            if not parsed:\n",
        "                continue\n",
        "            doc_id = parsed[0]\n",
        "            _, payload = proc.process(doc_id, parsed)\n",
        "            try:\n",
        "                obj = json.loads(payload)\n",
        "                docs.append(obj)\n",
        "            except json.JSONDecodeError:\n",
        "                # Should not happen, but skip invalid\n",
        "                continue\n",
        "            added += 1\n",
        "            if added >= limit:\n",
        "                break\n",
        "    return docs\n",
        "\n",
        "\n",
        "try:\n",
        "    documents: list[dict[str, Any]] = load_documents(DOCUMENTS_PATH, limit=100)\n",
        "    print(f\"Loaded {len(documents)} docs from {DOCUMENTS_PATH}\")\n",
        "except Exception as e:\n",
        "    documents = []\n",
        "    print(f\"Failed to load documents: {e}\")\n",
        "\n",
        "# Helper to get combined text\n",
        "\n",
        "def get_document_text(doc: dict[str, Any], include_title: bool = True, include_body: bool = True) -> str:\n",
        "    title = str(doc.get(Columns.title.value, \"\")) if include_title else \"\"\n",
        "    body = str(doc.get(Columns.body.value, \"\")) if include_body else \"\"\n",
        "    sep = \"\\n\\n\" if title and body else \"\"\n",
        "    return f\"{title}{sep}{body}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenizer builders and comparison utilities\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class SpacyOptions:\n",
        "    model: str = \"blank\"            # \"blank\" for spacy.blank(\"en\"), or a model name like \"en_core_web_sm\"\n",
        "    disable: list[str] | None = None # pipeline components to disable when loading a model\n",
        "    lowercase: bool = True\n",
        "    ascii_fold: bool = True\n",
        "    add_url_rule: bool = True\n",
        "    add_email_rule: bool = True\n",
        "    custom_infixes: list[str] | None = None  # e.g. [r\"\\.\", r\"-\", r\"_\", r\"\\/\", r\"\\:\"]\n",
        "\n",
        "\n",
        "def build_spacy_tokenizer(opts: SpacyOptions) -> SpacyTokenizer:\n",
        "    \"\"\"Build our SpacyTokenizer honoring basic normalization and tokenizer rules.\n",
        "    If opts.model == \"blank\", we create spacy.blank(\"en\"); otherwise spacy.load.\n",
        "    We optionally alter tokenizer URL/email matching and infix rules.\n",
        "    \"\"\"\n",
        "    import spacy\n",
        "    if opts.model in (None, \"\", \"blank\"):\n",
        "        nlp = spacy.blank(\"en\")\n",
        "    else:\n",
        "        try:\n",
        "            nlp = spacy.load(opts.model, disable=opts.disable or [\n",
        "                \"tagger\", \"parser\", \"ner\", \"lemmatizer\", \"attribute_ruler\", \"textcat\"\n",
        "            ])\n",
        "        except Exception:\n",
        "            nlp = spacy.blank(\"en\")\n",
        "\n",
        "    # Customize tokenizer rules if requested\n",
        "    from spacy.lang.char_classes import LIST_ELLIPSES, LIST_ICONS\n",
        "    from spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER\n",
        "    from spacy.tokenizer import Tokenizer\n",
        "    from spacy.util import compile_infix_regex\n",
        "\n",
        "    # Start with default tokenizer\n",
        "    tokenizer = nlp.tokenizer\n",
        "\n",
        "    # Adjust URL/email matching via token_match\n",
        "    token_match = tokenizer.token_match\n",
        "    if opts.add_url_rule or opts.add_email_rule:\n",
        "        import re\n",
        "        url_pattern = r\"https?://\\S+\" if opts.add_url_rule else None\n",
        "        email_pattern = r\"[\\w.+-]+@[\\w-]+\\.[\\w.-]+\" if opts.add_email_rule else None\n",
        "        patterns = [p for p in [url_pattern, email_pattern] if p]\n",
        "        if patterns:\n",
        "            combined = re.compile(\"|\".join(patterns))\n",
        "            def _token_match(text: str) -> Optional[re.Match[str]]:  # type: ignore\n",
        "                m = combined.match(text)\n",
        "                return m if m else (token_match(text) if token_match else None)\n",
        "            tokenizer.token_match = _token_match  # type: ignore\n",
        "\n",
        "    # Adjust infixes if provided\n",
        "    if opts.custom_infixes is not None:\n",
        "        infixes = tuple(opts.custom_infixes)\n",
        "        infix_re = compile_infix_regex(infixes)\n",
        "        tokenizer.infix_finditer = infix_re.finditer  # type: ignore\n",
        "\n",
        "    # Wrap into our SpacyTokenizer to leverage normalization and re-fallback\n",
        "    st = SpacyTokenizer(model=\"blank\", disable=None, lowercase=opts.lowercase, ascii_fold=opts.ascii_fold)\n",
        "    st.nlp = nlp\n",
        "    st.nlp.tokenizer = tokenizer\n",
        "    return st\n",
        "\n",
        "\n",
        "def compare_tokenizers(text: str, simple_tok: SimpleTokenizer, spacy_tok: SpacyTokenizer, custom_spacy_tok: SpacyTokenizer) -> dict[str, list[str]]:\n",
        "    return {\n",
        "        \"SimpleTokenizer\": simple_tok.tokenize(text),\n",
        "        \"SpacyTokenizer(default)\": spacy_tok.tokenize(text),\n",
        "        \"SpacyTokenizer(custom)\": custom_spacy_tok.tokenize(text),\n",
        "    }\n",
        "\n",
        "# Defaults\n",
        "default_simple = SimpleTokenizer(lowercase=True, ascii_fold=True)\n",
        "default_spacy = SpacyTokenizer(model=\"blank\", disable=None, lowercase=True, ascii_fold=True)\n",
        "\n",
        "# Example customizable options instance\n",
        "default_opts = SpacyOptions(\n",
        "    model=os.getenv(\"SPACY_MODEL\", \"blank\"),\n",
        "    disable=[c for c in (os.getenv(\"SPACY_DISABLE\", \"\").split(\",\") if os.getenv(\"SPACY_DISABLE\") else [])],\n",
        "    lowercase=True,\n",
        "    ascii_fold=True,\n",
        "    add_url_rule=True,\n",
        "    add_email_rule=True,\n",
        "    custom_infixes=[r\"\\.\", r\"-\", r\"_\", r\"\\/\", r\"\\:\"]\n",
        ")\n",
        "\n",
        "custom_spacy = build_spacy_tokenizer(default_opts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd8fdafc6b6046a193cfa36c96d73b00",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HBox(children=(IntSlider(value=0, description='doc idx', max=99), Checkbox(value=True, descriptâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b94517be32214cca99e66d480724c923",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Interactive UI (if ipywidgets available)\n",
        "\n",
        "def render_results(text: str, simple_tok: SimpleTokenizer, spacy_tok: SpacyTokenizer, custom_spacy_tok: SpacyTokenizer):\n",
        "    # Show full document text\n",
        "    print(f\"Document text ({len(text)} chars):\\n{text}\\n\")\n",
        "    comp = compare_tokenizers(text, simple_tok, spacy_tok, custom_spacy_tok)\n",
        "    rows = []\n",
        "    for name, tokens in comp.items():\n",
        "        rows.append({\n",
        "            \"Tokenizer\": name,\n",
        "            \"Num tokens\": len(tokens),\n",
        "            \"Tokens\": \" \".join(tokens),\n",
        "        })\n",
        "    df = pd.DataFrame(rows)\n",
        "    display(df)\n",
        "\n",
        "if HAS_WIDGETS and documents:\n",
        "    # Controls\n",
        "    doc_index = widgets.IntSlider(value=0, min=0, max=max(0, len(documents)-1), step=1, description=\"doc idx\")\n",
        "    include_title = widgets.Checkbox(value=True, description=\"include title\")\n",
        "    include_body = widgets.Checkbox(value=True, description=\"include body\")\n",
        "\n",
        "    spacy_model = widgets.Text(value=os.getenv(\"SPACY_MODEL\", \"blank\"), description=\"spacy model\")\n",
        "    spacy_disable = widgets.Text(value=os.getenv(\"SPACY_DISABLE\", \"\"), description=\"disable\")\n",
        "    lowercase = widgets.Checkbox(value=True, description=\"lowercase\")\n",
        "    ascii_fold = widgets.Checkbox(value=True, description=\"ascii fold\")\n",
        "    add_url = widgets.Checkbox(value=True, description=\"url rule\")\n",
        "    add_email = widgets.Checkbox(value=True, description=\"email rule\")\n",
        "    infixes = widgets.Text(value=r\"\\.|-|_|\\/|\\:\", description=\"infixes regex |\")\n",
        "\n",
        "    out = widgets.Output()\n",
        "\n",
        "    def _update(_=None):\n",
        "        out.clear_output()\n",
        "        with out:\n",
        "            # Build current custom spacy tokenizer\n",
        "            disable_list = [c for c in spacy_disable.value.split(\",\") if c.strip()]\n",
        "            opts = SpacyOptions(\n",
        "                model=spacy_model.value.strip() or \"blank\",\n",
        "                disable=disable_list or None,\n",
        "                lowercase=lowercase.value,\n",
        "                ascii_fold=ascii_fold.value,\n",
        "                add_url_rule=add_url.value,\n",
        "                add_email_rule=add_email.value,\n",
        "                custom_infixes=[p for p in infixes.value.split(\"|\") if p]\n",
        "            )\n",
        "            custom_tok = build_spacy_tokenizer(opts)\n",
        "            text = get_document_text(documents[doc_index.value], include_title.value, include_body.value)\n",
        "            render_results(text, default_simple, default_spacy, custom_tok)\n",
        "\n",
        "    for w in [doc_index, include_title, include_body, spacy_model, spacy_disable, lowercase, ascii_fold, add_url, add_email, infixes]:\n",
        "        w.observe(_update, names=[\"value\"])  # type: ignore\n",
        "\n",
        "    control_box = widgets.VBox([\n",
        "        widgets.HBox([doc_index, include_title, include_body]),\n",
        "        widgets.HBox([spacy_model, spacy_disable]),\n",
        "        widgets.HBox([lowercase, ascii_fold, add_url, add_email]),\n",
        "        infixes,\n",
        "    ])\n",
        "    display(control_box, out)\n",
        "    _update()\n",
        "else:\n",
        "    print(\"Widgets are unavailable or no documents loaded. See the fallback cell below.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fallback demo (no widgets)\n",
        "if not HAS_WIDGETS:\n",
        "    examples: list[str] = []\n",
        "    if documents:\n",
        "        # Take first 3 documents' title+body\n",
        "        for i in range(min(3, len(documents))):\n",
        "            examples.append(get_document_text(documents[i], True, True))\n",
        "    else:\n",
        "        examples = [\n",
        "            \"Hello world! Email me at foo.bar+baz@example.com or visit https://spacy.io/usage.\",\n",
        "            \"SpaCy v3.8's tokenizer: test-case with hyphenated-words_and_tokens/segments: 12:30pm.\",\n",
        "        ]\n",
        "\n",
        "    # Build a couple of custom variants\n",
        "    opts_a = SpacyOptions(model=\"blank\", custom_infixes=[r\"\\.\", r\"-\", r\"_\"], add_url_rule=True, add_email_rule=True)\n",
        "    opts_b = SpacyOptions(model=\"blank\", custom_infixes=[r\"\\-\"], add_url_rule=False, add_email_rule=False, lowercase=False, ascii_fold=False)\n",
        "\n",
        "    custom_a = build_spacy_tokenizer(opts_a)\n",
        "    custom_b = build_spacy_tokenizer(opts_b)\n",
        "\n",
        "    for idx, text in enumerate(examples):\n",
        "        print(f\"\\n=== Example {idx} ===\\n{text}\\n\")\n",
        "        comp = {\n",
        "            \"SimpleTokenizer\": default_simple.tokenize(text),\n",
        "            \"SpacyTokenizer(default)\": default_spacy.tokenize(text),\n",
        "            \"SpacyTokenizer(custom A)\": custom_a.tokenize(text),\n",
        "            \"SpacyTokenizer(custom B)\": custom_b.tokenize(text),\n",
        "        }\n",
        "        df = pd.DataFrame({\n",
        "            \"Tokenizer\": comp.keys(),\n",
        "            \"Num tokens\": [len(v) for v in comp.values()],\n",
        "            \"Tokens\": [\" \".join(v) for v in comp.values()],\n",
        "        })\n",
        "        display(df)\n",
        "else:\n",
        "    print(\"Widgets available; use the interactive controls above.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading config from configs/base.yaml.\n",
            "Connected to Redis at localhost:6379 db=0. Keys present: 46942\n",
            "Redis DB flushed.\n"
          ]
        }
      ],
      "source": [
        "import redis\n",
        "from utils.config import Config\n",
        "cfg = Config(load=True)\n",
        "\n",
        "\n",
        "# Configure connection via env vars or defaults\n",
        "REDIS_HOST = cfg.REDIS_HOST\n",
        "REDIS_PORT = cfg.REDIS_PORT\n",
        "REDIS_DB = 0\n",
        "REDIS_PASSWORD = None\n",
        "\n",
        "r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=REDIS_DB, password=REDIS_PASSWORD)\n",
        "\n",
        "n_keys = r.dbsize()\n",
        "\n",
        "print(f\"Connected to Redis at {REDIS_HOST}:{REDIS_PORT} db={REDIS_DB}. Keys present: {n_keys}\")\n",
        "\n",
        "# r.flushdb()\n",
        "print(\"Redis DB flushed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Key: token:180rotten (raw: b'token:180rotten')  Type: hash\n",
            "Value (raw repr):\n",
            "{b'D1761147': b'{\"tf\": 1, \"pos\": [11]}'}\n",
            "------------------------------------------------------------\n",
            "2. Key: token:brockhampton (raw: b'token:brockhampton')  Type: hash\n",
            "Value (raw repr):\n",
            "{b'D2580791': b'{\"tf\": 1, \"pos\": [13710]}'}\n",
            "------------------------------------------------------------\n",
            "3. Key: token:taxus (raw: b'token:taxus')  Type: hash\n",
            "Value (raw repr):\n",
            "{b'D2226663': b'{\"tf\": 1, \"pos\": [149]}'}\n",
            "------------------------------------------------------------\n",
            "4. Key: token:424 (raw: b'token:424')  Type: hash\n",
            "Value (raw repr):\n",
            "{b'D431186': b'{\"tf\": 1, \"pos\": [3961]}'}\n",
            "------------------------------------------------------------\n",
            "5. Key: token:berg (raw: b'token:berg')  Type: hash\n",
            "Value (raw repr):\n",
            "{b'D2580791': b'{\"tf\": 1, \"pos\": [12943]}'}\n",
            "------------------------------------------------------------\n",
            "6. Key: token:igl (raw: b'token:igl')  Type: hash\n",
            "Value (raw repr):\n",
            "{b'D806663': b'{\"tf\": 2, \"pos\": [490, 1219]}'}\n",
            "------------------------------------------------------------\n",
            "7. Key: token:773 (raw: b'token:773')  Type: hash\n",
            "Value (raw repr):\n",
            "{b'D1270543': b'{\"tf\": 1, \"pos\": [1259]}'}\n",
            "------------------------------------------------------------\n",
            "8. Key: token:efficacy (raw: b'token:efficacy')  Type: hash\n",
            "Value (raw repr):\n",
            "{b'D1367933': b'{\"tf\": 1, \"pos\": [807]}'}\n",
            "------------------------------------------------------------\n",
            "9. Key: token:shelters (raw: b'token:shelters')  Type: hash\n",
            "Value (raw repr):\n",
            "{b'D2240576': b'{\"tf\": 1, \"pos\": [2451]}', b'D3135451': b'{\"tf\": 1, \"pos\": [539]}'}\n",
            "------------------------------------------------------------\n",
            "10. Key: token:3512520 (raw: b'token:3512520')  Type: hash\n",
            "Value (raw repr):\n",
            "{b'D490350': b'{\"tf\": 2, \"pos\": [208, 235]}'}\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from typing import Any\n",
        "\n",
        "# Fetch up to 10 keys starting with \"token:\" and print their raw DB representation.\n",
        "# Relies on `r` (redis.Redis) already defined in an earlier cell.\n",
        "\n",
        "match = \"token:*\"\n",
        "def _decode_if_bytes(v: Any) -> Any:\n",
        "    try:\n",
        "        if isinstance(v, bytes):\n",
        "            return v  # keep bytes to show raw DB bytes\n",
        "        if isinstance(v, dict):\n",
        "            # keep bytes in dict keys/values if present\n",
        "            return v\n",
        "    except Exception:\n",
        "        pass\n",
        "    return v\n",
        "\n",
        "keys = []\n",
        "for k in r.scan_iter(match=match):\n",
        "    keys.append(k)\n",
        "    if len(keys) >= 10:\n",
        "        break\n",
        "\n",
        "if not keys:\n",
        "    print(f\"No keys matching '{match}' found.\")\n",
        "else:\n",
        "    for idx, key in enumerate(keys, 1):\n",
        "        # normalize display of key and type\n",
        "        key_display = key.decode() if isinstance(key, (bytes, bytearray)) else str(key)\n",
        "        key_repr = repr(key)\n",
        "        t = r.type(key)\n",
        "        t_str = t.decode() if isinstance(t, (bytes, bytearray)) else str(t)\n",
        "\n",
        "        # fetch raw value depending on type\n",
        "        if t_str in (\"string\", \"bytes\", \"basic-string\"):\n",
        "            val = r.get(key)\n",
        "        elif t_str == \"hash\":\n",
        "            val = r.hgetall(key)\n",
        "        elif t_str == \"list\":\n",
        "            val = r.lrange(key, 0, -1)\n",
        "        elif t_str == \"set\":\n",
        "            val = r.smembers(key)\n",
        "        elif t_str == \"zset\":\n",
        "            val = r.zrange(key, 0, -1, withscores=True)\n",
        "        elif t_str == \"stream\":\n",
        "            val = r.xrange(key, count=100)\n",
        "        else:\n",
        "            # fallback: try GET, otherwise raw type info\n",
        "            try:\n",
        "                val = r.get(key)\n",
        "            except Exception:\n",
        "                val = f\"<unhandled type: {t_str}>\"\n",
        "\n",
        "        print(f\"{idx}. Key: {key_display} (raw: {key_repr})  Type: {t_str}\")\n",
        "        print(\"Value (raw repr):\")\n",
        "        print(repr(_decode_if_bytes(val)))\n",
        "        print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'D687756': '{\"tf\": 2, \"pos\": [6421, 12201]}',\n",
              " 'D1598311': '{\"tf\": 1, \"pos\": [152]}',\n",
              " 'D3141039': '{\"tf\": 1, \"pos\": [22]}',\n",
              " 'D3395201': '{\"tf\": 1, \"pos\": [454]}',\n",
              " 'D574800': '{\"tf\": 1, \"pos\": [235]}',\n",
              " 'D2632012': '{\"tf\": 2, \"pos\": [446, 5067]}',\n",
              " 'D2146220': '{\"tf\": 1, \"pos\": [6051]}',\n",
              " 'D2226663': '{\"tf\": 2, \"pos\": [152, 389]}',\n",
              " 'D1887456': '{\"tf\": 1, \"pos\": [35]}',\n",
              " 'D398522': '{\"tf\": 10, \"pos\": [9, 13, 93, 141, 174, 207, 415, 422, 509, 1050]}',\n",
              " 'D1745929': '{\"tf\": 2, \"pos\": [33, 38]}',\n",
              " 'D2520478': '{\"tf\": 1, \"pos\": [103]}',\n",
              " 'D339947': '{\"tf\": 2, \"pos\": [162, 169]}',\n",
              " 'D1142478': '{\"tf\": 1, \"pos\": [391]}',\n",
              " 'D1647345': '{\"tf\": 1, \"pos\": [125]}',\n",
              " 'D2805086': '{\"tf\": 1, \"pos\": [291]}',\n",
              " 'D1725154': '{\"tf\": 1, \"pos\": [17]}',\n",
              " 'D2499684': '{\"tf\": 1, \"pos\": [867]}',\n",
              " 'D1533134': '{\"tf\": 1, \"pos\": [669]}',\n",
              " 'D467137': '{\"tf\": 1, \"pos\": [51]}',\n",
              " 'D1268130': '{\"tf\": 1, \"pos\": [262]}',\n",
              " 'D297612': '{\"tf\": 1, \"pos\": [644]}',\n",
              " 'D2761080': '{\"tf\": 1, \"pos\": [60]}',\n",
              " 'D2362419': '{\"tf\": 14, \"pos\": [7, 21, 23, 81, 84, 85, 117, 120, 122, 129, 132, 149, 180, 188]}',\n",
              " 'D2380450': '{\"tf\": 1, \"pos\": [232]}'}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = r.hgetall(\"token:tree\")\n",
        "\n",
        "result = {rkey.decode(): r.decode() for rkey, r in result.items()}\n",
        "result"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sea-wise-2025-26",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
